# src/core/tools/trend_tools.py
from typing import Dict, Any, List
import structlog
from mcp.types import Tool, TextContent
import json
from .base_tools import BaseTools
from src.utils.error_handler import handle_tool_error

logger = structlog.get_logger()

class TrendTools(BaseTools):
    """è¶‹åŠ¿åˆ†æå·¥å…· - æ”¯æŒJSONæ ¼å¼è¿”å›"""
    
    def get_tools(self) -> List[Tool]:
        """è·å–å·¥å…·å®šä¹‰"""
        return [
            Tool(
                name="get_trending_papers",
                description="è·å–çƒ­é—¨è®ºæ–‡ï¼Œåˆ†æå½“å‰å­¦æœ¯çƒ­ç‚¹",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "time_window": {
                            "type": "string",
                            "enum": ["week", "month", "year"],
                            "default": "month",
                            "description": "æ—¶é—´çª—å£ï¼šweek(ä¸€å‘¨), month(ä¸€æœˆ), year(ä¸€å¹´)"
                        },
                        "limit": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 100,
                            "default": 20,
                            "description": "è¿”å›ç»“æœæ•°é‡"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "json"],
                            "default": "markdown",
                            "description": "è¿”å›æ ¼å¼ï¼šmarkdown(æ ¼å¼åŒ–æ˜¾ç¤º) æˆ– json(åŸå§‹æ•°æ®)"
                        }
                    }
                }
            ),
            Tool(
                name="get_top_keywords",
                description="è·å–çƒ­é—¨å…³é”®è¯ï¼Œåˆ†æç ”ç©¶çƒ­ç‚¹è¯é¢˜",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "limit": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 100,
                            "default": 20,
                            "description": "è¿”å›ç»“æœæ•°é‡"
                        },
                        "time_range": {
                            "type": "string",
                            "description": "æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ï¼šYYYY-YYYY"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "json"],
                            "default": "markdown",
                            "description": "è¿”å›æ ¼å¼ï¼šmarkdown(æ ¼å¼åŒ–æ˜¾ç¤º) æˆ– json(åŸå§‹æ•°æ®)"
                        }
                    }
                }
            ),
            Tool(
                name="analyze_domain_trends",
                description="åˆ†æç‰¹å®šé¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "domain": {
                            "type": "string",
                            "description": "ç ”ç©¶é¢†åŸŸåç§°"
                        },
                        "time_range": {
                            "type": "string",
                            "default": "2020-2024",
                            "description": "æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ï¼šYYYY-YYYY"
                        },
                        "metrics": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "enum": ["publication_count", "citation_count", "author_count"]
                            },
                            "default": ["publication_count"],
                            "description": "åˆ†ææŒ‡æ ‡"
                        },
                        "granularity": {
                            "type": "string",
                            "enum": ["year", "quarter", "month"],
                            "default": "year",
                            "description": "æ—¶é—´ç²’åº¦"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "json"],
                            "default": "markdown",
                            "description": "è¿”å›æ ¼å¼ï¼šmarkdown(æ ¼å¼åŒ–æ˜¾ç¤º) æˆ– json(åŸå§‹æ•°æ®)"
                        }
                    },
                    "required": ["domain"]
                }
            ),
            Tool(
                name="analyze_research_landscape",
                description="åˆ†æç ”ç©¶é¢†åŸŸå…¨æ™¯ï¼ŒåŒ…æ‹¬çƒ­ç‚¹è¯é¢˜ã€æ´»è·ƒä½œè€…ã€æ–°å…´è¶‹åŠ¿ç­‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "domain": {
                            "type": "string",
                            "description": "ç ”ç©¶é¢†åŸŸåç§°"
                        },
                        "analysis_dimensions": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "enum": ["topics", "authors", "trends", "institutions"]
                            },
                            "default": ["topics", "authors", "trends"],
                            "description": "åˆ†æç»´åº¦"
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "json"],
                            "default": "markdown",
                            "description": "è¿”å›æ ¼å¼ï¼šmarkdown(æ ¼å¼åŒ–æ˜¾ç¤º) æˆ– json(åŸå§‹æ•°æ®)"
                        }
                    },
                    "required": ["domain"]
                }
            )
        ]
    
    @handle_tool_error
    async def get_trending_papers(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·å–çƒ­é—¨è®ºæ–‡å·¥å…· - æ”¯æŒJSONæ ¼å¼"""
        time_window = arguments.get("time_window", "year")
        limit = arguments.get("limit", 2)
        return_format = arguments.get("format", "json")

        logger.info("Getting trending papers", time_window=time_window, limit=limit, format=return_format)
        
        try:
            raw_result = await self.go_client.get_trending_papers(
                time_window=time_window,
                limit=limit
            )
            
            if return_format == "json":
                # è¿”å›åŸå§‹ JSON
                # return [TextContent(type="text", text=json.dumps(raw_result, ensure_ascii=False, indent=2))]
                            # è¿”å›åŸå§‹ JSON
                json_text = json.dumps(raw_result, ensure_ascii=False, indent=2)
                # è¾“å‡ºåˆ°æ—¥å¿—
                logger.debug("Returning JSON result", json_content=json_text)
                # æˆ–è€…ä½¿ç”¨ info çº§åˆ«
                logger.info("Trending papers JSON result", 
                        json_length=len(json_text))
                # å¦‚æœéœ€è¦å®Œæ•´å†…å®¹ï¼Œå¯ä»¥å•ç‹¬è®°å½•
                logger.debug(f"Full JSON response:\n{json_text}")
                return [TextContent(type="text", text=json_text)]
            else:
                # è¿”å›æ ¼å¼åŒ–çš„ Markdownï¼ˆé»˜è®¤ï¼‰
                content = self._format_trending_papers(raw_result, time_window)
                return [TextContent(type="text", text=content)]
              
        except Exception as e:
            logger.error("Get trending papers failed", error=str(e))
            error_content = self._format_error_response(str(e), "è·å–çƒ­é—¨è®ºæ–‡")
            return [TextContent(type="text", text=error_content)]
    
    @handle_tool_error
    async def get_top_keywords(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·å–çƒ­é—¨å…³é”®è¯å·¥å…· - æ”¯æŒJSONæ ¼å¼"""
        limit = arguments.get("limit", 20)
        time_range = arguments.get("time_range")
        return_format = arguments.get("format", "json")
        
        logger.info("Getting top keywords", limit=limit, time_range=time_range, format=return_format)
        
        try:
            raw_result = await self.go_client.get_top_keywords()
            
            # å¦‚æœæœ‰æ—¶é—´èŒƒå›´é™åˆ¶ï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿‡æ»¤
            if time_range:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤é€»è¾‘
                pass
            
            if return_format == "json":
                # è¿”å›åŸå§‹ JSON
                return [TextContent(type="text", text=json.dumps(raw_result, ensure_ascii=False, indent=2))]
            else:
                # è¿”å›æ ¼å¼åŒ–çš„ Markdownï¼ˆé»˜è®¤ï¼‰
                content = self._format_top_keywords(raw_result, limit)
                return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Get top keywords failed", error=str(e))
            error_content = self._format_error_response(str(e), "è·å–çƒ­é—¨å…³é”®è¯")
            return [TextContent(type="text", text=error_content)]
    
    @handle_tool_error
    async def analyze_domain_trends(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """åˆ†æé¢†åŸŸè¶‹åŠ¿å·¥å…· - æ”¯æŒJSONæ ¼å¼"""
        domain = arguments["domain"]
        time_range = arguments.get("time_range", "2020-2024")
        metrics = arguments.get("metrics", ["publication_count"])
        granularity = arguments.get("granularity", "year")
        return_format = arguments.get("format", "json")
        
        logger.info(
            "Analyzing domain trends", 
            domain=domain, 
            time_range=time_range, 
            metrics=metrics,
            granularity=granularity,
            format=return_format
        )
        
        try:
            raw_result = await self.go_client.get_research_trends(
                domain=domain,
                time_range=time_range,
                metrics=metrics,
                granularity=granularity
            )
            
            # if return_format == "json":
                # è¿”å›åŸå§‹ JSON
            return [TextContent(type="text", text=json.dumps(raw_result, ensure_ascii=False, indent=2))]
            # else:
            #     # è¿”å›æ ¼å¼åŒ–çš„ Markdownï¼ˆé»˜è®¤ï¼‰
            #     content = self._format_domain_trends(raw_result, domain)
            #     return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Analyze domain trends failed", error=str(e))
            error_content = self._format_error_response(str(e), "åˆ†æé¢†åŸŸè¶‹åŠ¿")
            return [TextContent(type="text", text=error_content)]
    
    @handle_tool_error
    async def analyze_research_landscape(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """åˆ†æç ”ç©¶å…¨æ™¯å·¥å…· - æ”¯æŒJSONæ ¼å¼"""
        domain = arguments["domain"]
        analysis_dimensions = arguments.get("analysis_dimensions", ["topics", "authors", "trends"])
        return_format = arguments.get("format", "json")
        
        logger.info(
            "Analyzing research landscape", 
            domain=domain, 
            analysis_dimensions=analysis_dimensions,
            format=return_format
        )
        
        try:
            raw_result = await self.go_client.analyze_research_landscape(
                domain=domain,
                analysis_dimensions=analysis_dimensions
            )
            
            # if return_format == "json":
            # è¿”å›åŸå§‹ JSON
            return [TextContent(type="text", text=json.dumps(raw_result, ensure_ascii=False, indent=2))]
            # else:
            #     # è¿”å›æ ¼å¼åŒ–çš„ Markdownï¼ˆé»˜è®¤ï¼‰
            #     content = self._format_research_landscape(raw_result, domain)
            #     return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Analyze research landscape failed", error=str(e))
            error_content = self._format_error_response(str(e), "åˆ†æç ”ç©¶å…¨æ™¯")
            return [TextContent(type="text", text=error_content)]
    
    def _format_trending_papers(self, raw_result: Dict[str, Any], time_window: str) -> str:
        """æ ¼å¼åŒ–çƒ­é—¨è®ºæ–‡ç»“æœ - æ ¹æ®å®é™…APIè¿”å›æ•°æ®è°ƒæ•´"""
        papers = raw_result.get("trending_papers", [])
        count = raw_result.get("count", len(papers))
        
        time_window_cn = {
            "week": "æœ¬å‘¨",
            "month": "æœ¬æœˆ", 
            "year": "æœ¬å¹´"
        }.get(time_window, time_window)
        
        content = f"# ğŸ“ˆ {time_window_cn}çƒ­é—¨è®ºæ–‡\n\n"
        content += f"**æ—¶é—´èŒƒå›´**: {time_window_cn}\n"
        content += f"**è®ºæ–‡æ•°é‡**: {count}\n\n"
        
        if not papers:
            content += f"{time_window_cn}æš‚æ— çƒ­é—¨è®ºæ–‡æ•°æ®ã€‚\n"
            return content
        
        content += f"## ğŸ† çƒ­é—¨è®ºæ–‡æ’è¡Œæ¦œ\n\n"
        
        for i, paper in enumerate(papers, 1):
            title = self._safe_get_str(paper, 'title', 'Unknown Title')
            content += f"### {i}. {title}\n\n"
            
            # åŸºæœ¬ä¿¡æ¯
            content += self._format_paper_basic_info(paper)
            
            # çƒ­åº¦æŒ‡æ ‡ - ä½¿ç”¨å®é™…å­—æ®µå
            popularity_score = paper.get('popularity_score')
            if popularity_score is not None:
                content += f"**ğŸ”¥ çƒ­åº¦è¯„åˆ†**: {popularity_score:.3f}\n"
            
            # å¼•ç”¨æ•°
            citations = self._safe_get_int(paper, 'citations')
            if citations > 0:
                content += f"**ğŸ“Š å¼•ç”¨æ•°**: {citations}\n"
            
            # å‘è¡¨ä¿¡æ¯
            published_year = self._safe_get_int(paper, 'published_year')
            if published_year > 0:
                content += f"**ğŸ“… å‘è¡¨å¹´ä»½**: {published_year}\n"
            
            published_at = self._safe_get_str(paper, 'published_at')
            if published_at:
                try:
                    from datetime import datetime
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    formatted_date = dt.strftime('%Yå¹´%mæœˆ%dæ—¥')
                    content += f"**ğŸ“… å‘è¡¨æ—¥æœŸ**: {formatted_date}\n"
                except:
                    content += f"**ğŸ“… å‘è¡¨æ—¥æœŸ**: {published_at}\n"
            
            # æœŸåˆŠ/ä¼šè®®ä¿¡æ¯
            venue_name = self._safe_get_str(paper, 'venue_name')
            venue_id = self._safe_get_str(paper, 'venue_id')
            if venue_name:
                content += f"**ğŸ“– å‘è¡¨äº**: {venue_name}"
                if venue_id and venue_id != venue_name:
                    content += f" ({venue_id})"
                content += "\n"
            
            # DOI å’Œé“¾æ¥
            doi = self._safe_get_str(paper, 'doi')
            if doi:
                content += f"**ğŸ”— DOI**: {doi}\n"
            
            url = self._safe_get_str(paper, 'url')
            if url:
                content += f"**ğŸ“„ è®ºæ–‡é“¾æ¥**: {url}\n"
            
            # ç¼©ç•¥å›¾
            img_url = self._safe_get_str(paper, 'img_url')
            if img_url:
                content += f"**ğŸ–¼ï¸ ç¼©ç•¥å›¾**: {img_url}\n"
            
            # ä½œè€…ä¿¡æ¯
            authors = paper.get('authors', [])
            if authors:
                if isinstance(authors[0], str):
                    # ä½œè€…æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                    content += f"**ğŸ‘¥ ä½œè€…**: {', '.join(authors)}\n"
                else:
                    # ä½œè€…æ˜¯å¯¹è±¡åˆ—è¡¨
                    author_names = []
                    for author in authors:
                        if isinstance(author, dict):
                            name = author.get('name', str(author))
                            author_names.append(name)
                        else:
                            author_names.append(str(author))
                    content += f"**ğŸ‘¥ ä½œè€…**: {', '.join(author_names)}\n"
            
            # å…³é”®è¯
            keywords = paper.get('keywords', [])
            if keywords and keywords != [""]:
                valid_keywords = [kw for kw in keywords if kw.strip()]
                if valid_keywords:
                    content += f"**ğŸ·ï¸ å…³é”®è¯**: {self._format_keywords(valid_keywords, max_count=5)}\n"
            
            # æ‘˜è¦
            abstract = self._safe_get_str(paper, 'abstract')
            if abstract:
                content += f"**ğŸ“ æ‘˜è¦**: {self._truncate_text(abstract, 200)}\n"
            
            # è®ºæ–‡ID
            paper_id = self._safe_get_str(paper, 'id')
            if paper_id:
                content += f"**ğŸ†” è®ºæ–‡ID**: `{paper_id}`\n"
            
            # æ·»åŠ çƒ­åº¦ç­‰çº§æŒ‡ç¤ºå™¨
            if popularity_score is not None:
                if popularity_score >= 0.8:
                    content += "**ğŸ”¥ğŸ”¥ğŸ”¥ è¶…é«˜çƒ­åº¦**\n"
                elif popularity_score >= 0.6:
                    content += "**ğŸ”¥ğŸ”¥ é«˜çƒ­åº¦**\n"
                elif popularity_score >= 0.4:
                    content += "**ğŸ”¥ ä¸­ç­‰çƒ­åº¦**\n"
                else:
                    content += "**ğŸ“ˆ æ–°å…´çƒ­åº¦**\n"
            
            content += "\n---\n\n"
        
        # æ·»åŠ ç»Ÿè®¡åˆ†æ
        if len(papers) >= 3:
            content += "## ğŸ“Š çƒ­é—¨è®ºæ–‡åˆ†æ\n\n"
            
            # çƒ­åº¦åˆ†æ
            avg_popularity = sum(p.get('popularity_score', 0) for p in papers) / len(papers)
            content += f"- **å¹³å‡çƒ­åº¦è¯„åˆ†**: {avg_popularity:.3f}\n"
            
            # å¼•ç”¨åˆ†æ
            total_citations = sum(self._safe_get_int(p, 'citations') for p in papers)
            avg_citations = total_citations / len(papers)
            content += f"- **æ€»å¼•ç”¨æ•°**: {total_citations}\n"
            content += f"- **å¹³å‡å¼•ç”¨æ•°**: {avg_citations:.1f}\n"
            
            # å¹´ä»½åˆ†æ
            years = [self._safe_get_int(p, 'published_year') for p in papers if self._safe_get_int(p, 'published_year') > 0]
            if years:
                latest_year = max(years)
                earliest_year = min(years)
                content += f"- **å‘è¡¨å¹´ä»½èŒƒå›´**: {earliest_year} - {latest_year}\n"
            
            # æœŸåˆŠåˆ†æ
            venues = [self._safe_get_str(p, 'venue_name') for p in papers if self._safe_get_str(p, 'venue_name')]
            if venues:
                venue_counts = {}
                for venue in venues:
                    venue_counts[venue] = venue_counts.get(venue, 0) + 1
                top_venues = sorted(venue_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                content += f"- **ä¸»è¦å‘è¡¨æœŸåˆŠ**: {', '.join([f'{v}({c}ç¯‡)' for v, c in top_venues])}\n"
            
            # å…³é”®è¯åˆ†æ
            all_keywords = []
            for paper in papers:
                keywords = paper.get('keywords', [])
                if keywords and keywords != [""]:
                    all_keywords.extend([kw for kw in keywords if kw.strip()])
            
            if all_keywords:
                keyword_counts = {}
                for kw in all_keywords:
                    keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
                top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                content += f"- **çƒ­é—¨å…³é”®è¯**: {', '.join([f'{kw}({c})' for kw, c in top_keywords])}\n"
            
            content += "\n"
            # æ·»åŠ ä½¿ç”¨æç¤º
            content += "## ğŸ’¡ ä½¿ç”¨æç¤º\n\n"
            content += "- ç‚¹å‡»è®ºæ–‡IDå¯ä»¥è·å–è¯¦ç»†ä¿¡æ¯\n"
            content += "- çƒ­åº¦è¯„åˆ†åŸºäºå¤šä¸ªå› ç´ ï¼šå¼•ç”¨æ•°ã€å‘è¡¨æ—¶é—´ã€å…³æ³¨åº¦ç­‰\n"
            content += "- å¯ä»¥ä½¿ç”¨å…³é”®è¯è¿›ä¸€æ­¥æœç´¢ç›¸å…³è®ºæ–‡\n\n"
        
        return content

    def _format_paper_basic_info(self, paper: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è®ºæ–‡åŸºæœ¬ä¿¡æ¯çš„è¾…åŠ©æ–¹æ³•"""
        info = ""
        
        # è¿™ä¸ªæ–¹æ³•ä¿æŒç®€å•ï¼Œä¸»è¦ä¿¡æ¯åœ¨ä¸»æ–¹æ³•ä¸­å¤„ç†
        return info

    def _format_keywords(self, keywords: List[str], max_count: int = 5) -> str:
        """æ ¼å¼åŒ–å…³é”®è¯åˆ—è¡¨"""
        if not keywords:
            return "æ— "
        
        # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        display_keywords = keywords[:max_count]
        formatted = ', '.join(f'`{kw}`' for kw in display_keywords)
        
        if len(keywords) > max_count:
            formatted += f" ç­‰{len(keywords)}ä¸ªå…³é”®è¯"
        
        return formatted

    def _truncate_text(self, text: str, max_length: int = 150) -> str:
        """æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦"""
        if not text:
            return ""
        
        if len(text) <= max_length:
            return text
        
        # åœ¨å•è¯è¾¹ç•Œæˆªæ–­ï¼ˆå¯¹äºè‹±æ–‡ï¼‰
        truncated = text[:max_length]
        
        # å°è¯•åœ¨å¥å·ã€æ„Ÿå¹å·æˆ–é—®å·å¤„æˆªæ–­
        for punct in ['. ', '! ', '? ']:
            last_punct = truncated.rfind(punct)
            if last_punct > max_length * 0.7:  # è‡³å°‘ä¿ç•™70%çš„é•¿åº¦
                return truncated[:last_punct + 1] + "..."
        
        # å°è¯•åœ¨ç©ºæ ¼å¤„æˆªæ–­
        last_space = truncated.rfind(' ')
        if last_space > max_length * 0.8:  # è‡³å°‘ä¿ç•™80%çš„é•¿åº¦
            return truncated[:last_space] + "..."
        
        return truncated + "..."

    def _safe_get_str(self, data: Dict[str, Any], key: str, default: str = "") -> str:
        """å®‰å…¨è·å–å­—ç¬¦ä¸²å€¼"""
        value = data.get(key, default)
        return str(value) if value is not None else default

    def _safe_get_int(self, data: Dict[str, Any], key: str, default: int = 0) -> int:
        """å®‰å…¨è·å–æ•´æ•°å€¼"""
        value = data.get(key, default)
        try:
            return int(value) if value is not None else default
        except (ValueError, TypeError):
            return default


    
    def _format_top_keywords(self, raw_result: Dict[str, Any], limit: int) -> str:
        """æ ¼å¼åŒ–çƒ­é—¨å…³é”®è¯ç»“æœ - æ ¹æ®å®é™…APIè¿”å›æ•°æ®è°ƒæ•´"""
        keywords = raw_result.get("keywords", [])
        count = raw_result.get("count", len(keywords))
        
        content = f"# ğŸ·ï¸ çƒ­é—¨ç ”ç©¶å…³é”®è¯\n\n"
        content += f"**å…³é”®è¯æ€»æ•°**: {count}\n"
        content += f"**æ˜¾ç¤ºæ•°é‡**: {min(limit, len(keywords))}\n\n"
        
        if not keywords:
            content += "æš‚æ— å…³é”®è¯æ•°æ®ã€‚\n"
            return content
        
        # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        display_keywords = keywords[:limit]
        
        content += f"## ğŸ“Š å…³é”®è¯æ’è¡Œæ¦œ (æŒ‰è®ºæ–‡æ•°é‡æ’åº)\n\n"
        
        # è®¡ç®—æœ€å¤§è®ºæ–‡æ•°é‡ç”¨äºç»˜åˆ¶ç®€å•çš„æ¡å½¢å›¾
        max_count = max(kw.get('paper_count', 0) for kw in display_keywords) if display_keywords else 0
        
        for i, keyword_data in enumerate(display_keywords, 1):
            keyword = keyword_data.get('keyword', 'Unknown')
            paper_count = keyword_data.get('paper_count', 0)
            
            # è®¡ç®—ç›¸å¯¹çƒ­åº¦ï¼ˆç”¨äºæ˜¾ç¤ºæ¡å½¢å›¾ï¼‰
            if max_count > 0:
                relative_heat = paper_count / max_count
                bar_length = int(relative_heat * 20)  # æœ€å¤§20ä¸ªå­—ç¬¦çš„æ¡å½¢å›¾
                heat_bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            else:
                heat_bar = "â–‘" * 20
            
            # æ·»åŠ çƒ­åº¦ç­‰çº§
            if paper_count >= max_count * 0.8:
                heat_level = "ğŸ”¥ğŸ”¥ğŸ”¥"
            elif paper_count >= max_count * 0.5:
                heat_level = "ğŸ”¥ğŸ”¥"
            elif paper_count >= max_count * 0.2:
                heat_level = "ğŸ”¥"
            else:
                heat_level = "ğŸ“ˆ"
            
            content += f"### {i}. `{keyword}` {heat_level}\n\n"
            content += f"**ğŸ“„ è®ºæ–‡æ•°é‡**: {paper_count}\n"
            content += f"**ğŸ“Š çƒ­åº¦æ¡**: {heat_bar} ({relative_heat:.1%})\n"
            
            # è§£æå…³é”®è¯ç±»å‹ï¼ˆå¦‚æœæ˜¯å­¦ç§‘åˆ†ç±»ï¼‰
            keyword_info = self._parse_keyword_info(keyword)
            if keyword_info:
                content += f"**ğŸ« é¢†åŸŸ**: {keyword_info}\n"
            
            content += "\n---\n\n"
        
        # æ·»åŠ ç»Ÿè®¡åˆ†æ
        if len(display_keywords) >= 3:
            content += "## ğŸ“ˆ å…³é”®è¯åˆ†æ\n\n"
            
            # è®ºæ–‡æ•°é‡ç»Ÿè®¡
            total_papers = sum(kw.get('paper_count', 0) for kw in display_keywords)
            avg_papers = total_papers / len(display_keywords)
            content += f"- **æ€»è®ºæ–‡æ•°**: {total_papers}\n"
            content += f"- **å¹³å‡è®ºæ–‡æ•°**: {avg_papers:.1f}\n"
            content += f"- **æœ€çƒ­å…³é”®è¯**: `{display_keywords[0].get('keyword', 'N/A')}` ({display_keywords[0].get('paper_count', 0)} ç¯‡è®ºæ–‡)\n"
            
            # å­¦ç§‘åˆ†å¸ƒåˆ†æ
            subject_stats = self._analyze_subject_distribution(display_keywords)
            if subject_stats:
                content += f"- **ä¸»è¦å­¦ç§‘é¢†åŸŸ**: {', '.join([f'{subj}({count}ä¸ªå…³é”®è¯)' for subj, count in subject_stats[:5]])}\n"
            
            # çƒ­åº¦åˆ†å¸ƒ
            high_heat = len([kw for kw in display_keywords if kw.get('paper_count', 0) >= max_count * 0.5])
            medium_heat = len([kw for kw in display_keywords if max_count * 0.2 <= kw.get('paper_count', 0) < max_count * 0.5])
            low_heat = len([kw for kw in display_keywords if kw.get('paper_count', 0) < max_count * 0.2])
            
            content += f"- **çƒ­åº¦åˆ†å¸ƒ**: é«˜çƒ­åº¦({high_heat}ä¸ª) | ä¸­ç­‰çƒ­åº¦({medium_heat}ä¸ª) | æ–°å…´çƒ­åº¦({low_heat}ä¸ª)\n"
            
            content += "\n"
        
        # æ·»åŠ ä½¿ç”¨æç¤º
        content += "## ğŸ’¡ ä½¿ç”¨æç¤º\n\n"
        content += "- å¯ä»¥ä½¿ç”¨è¿™äº›å…³é”®è¯æœç´¢ç›¸å…³è®ºæ–‡\n"
        content += "- `cs.*` è¡¨ç¤ºè®¡ç®—æœºç§‘å­¦ç›¸å…³é¢†åŸŸ\n"
        content += "- `physics.*` è¡¨ç¤ºç‰©ç†å­¦ç›¸å…³é¢†åŸŸ\n"
        content += "- æ•°å­—è¶Šå¤§è¡¨ç¤ºè¯¥é¢†åŸŸçš„ç ”ç©¶è¶Šæ´»è·ƒ\n\n"
        
        return content

    def _parse_keyword_info(self, keyword: str) -> str:
        """è§£æå…³é”®è¯ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å­¦ç§‘åˆ†ç±»ä»£ç """
        if not keyword:
            return ""
        
        # å¸¸è§çš„å­¦ç§‘åˆ†ç±»æ˜ å°„
        subject_mapping = {
            # è®¡ç®—æœºç§‘å­¦
            "cs.CR": "è®¡ç®—æœºç§‘å­¦ - å¯†ç å­¦ä¸å®‰å…¨",
            "cs.SE": "è®¡ç®—æœºç§‘å­¦ - è½¯ä»¶å·¥ç¨‹", 
            "cs.PL": "è®¡ç®—æœºç§‘å­¦ - ç¼–ç¨‹è¯­è¨€",
            "cs.AR": "è®¡ç®—æœºç§‘å­¦ - ç¡¬ä»¶æ¶æ„",
            "cs.DL": "è®¡ç®—æœºç§‘å­¦ - æ•°å­—å›¾ä¹¦é¦†",
            "cs.AI": "è®¡ç®—æœºç§‘å­¦ - äººå·¥æ™ºèƒ½",
            "cs.LG": "è®¡ç®—æœºç§‘å­¦ - æœºå™¨å­¦ä¹ ",
            "cs.CV": "è®¡ç®—æœºç§‘å­¦ - è®¡ç®—æœºè§†è§‰",
            "cs.NI": "è®¡ç®—æœºç§‘å­¦ - ç½‘ç»œä¸äº’è”ç½‘æ¶æ„",
            "cs.DC": "è®¡ç®—æœºç§‘å­¦ - åˆ†å¸ƒå¼ã€å¹¶è¡Œä¸é›†ç¾¤è®¡ç®—",
            "cs.DB": "è®¡ç®—æœºç§‘å­¦ - æ•°æ®åº“",
            "cs.IR": "è®¡ç®—æœºç§‘å­¦ - ä¿¡æ¯æ£€ç´¢",
            "cs.HC": "è®¡ç®—æœºç§‘å­¦ - äººæœºäº¤äº’",
            "cs.CL": "è®¡ç®—æœºç§‘å­¦ - è®¡ç®—ä¸è¯­è¨€",
            "cs.GT": "è®¡ç®—æœºç§‘å­¦ - è®¡ç®—æœºç§‘å­¦ä¸åšå¼ˆè®º",
            
            # ç‰©ç†å­¦
            "physics.ed-ph": "ç‰©ç†å­¦ - ç‰©ç†æ•™è‚²",
            "physics.gen-ph": "ç‰©ç†å­¦ - æ™®é€šç‰©ç†",
            "physics.comp-ph": "ç‰©ç†å­¦ - è®¡ç®—ç‰©ç†",
            
            # æ•°å­¦
            "math.CO": "æ•°å­¦ - ç»„åˆæ•°å­¦",
            "math.LO": "æ•°å­¦ - é€»è¾‘",
            "math.ST": "æ•°å­¦ - ç»Ÿè®¡ç†è®º",
            
            # å…¶ä»–å¸¸è§é¢†åŸŸ
            "econ.EM": "ç»æµå­¦ - è®¡é‡ç»æµå­¦",
            "stat.ML": "ç»Ÿè®¡å­¦ - æœºå™¨å­¦ä¹ ",
            "q-bio.QM": "å®šé‡ç”Ÿç‰©å­¦ - å®šé‡æ–¹æ³•",
        }
        
        # ç›´æ¥åŒ¹é…
        if keyword in subject_mapping:
            return subject_mapping[keyword]
        
        # æ¨¡ç³ŠåŒ¹é…ä¸»è¦å­¦ç§‘
        if keyword.startswith("cs."):
            return f"è®¡ç®—æœºç§‘å­¦ - {keyword[3:].upper()}"
        elif keyword.startswith("physics."):
            return f"ç‰©ç†å­¦ - {keyword[8:].replace('-', ' ').title()}"
        elif keyword.startswith("math."):
            return f"æ•°å­¦ - {keyword[5:].upper()}"
        elif keyword.startswith("stat."):
            return f"ç»Ÿè®¡å­¦ - {keyword[5:].upper()}"
        elif keyword.startswith("econ."):
            return f"ç»æµå­¦ - {keyword[5:].upper()}"
        elif keyword.startswith("q-bio."):
            return f"å®šé‡ç”Ÿç‰©å­¦ - {keyword[6:].upper()}"
        
        return ""

    def _analyze_subject_distribution(self, keywords: List[Dict[str, Any]]) -> List[tuple]:
        """åˆ†æå­¦ç§‘åˆ†å¸ƒ"""
        subject_counts = {}
        
        for kw_data in keywords:
            keyword = kw_data.get('keyword', '')
            if '.' in keyword:
                subject = keyword.split('.')[0]
                subject_counts[subject] = subject_counts.get(subject, 0) + 1
        
        # å­¦ç§‘åç§°æ˜ å°„
        subject_names = {
            'cs': 'è®¡ç®—æœºç§‘å­¦',
            'physics': 'ç‰©ç†å­¦',
            'math': 'æ•°å­¦',
            'stat': 'ç»Ÿè®¡å­¦',
            'econ': 'ç»æµå­¦',
            'q-bio': 'å®šé‡ç”Ÿç‰©å­¦'
        }
        
        # è½¬æ¢ä¸ºå‹å¥½åç§°å¹¶æ’åº
        result = []
        for subject, count in sorted(subject_counts.items(), key=lambda x: x[1], reverse=True):
            friendly_name = subject_names.get(subject, subject.upper())
            result.append((friendly_name, count))
        
        return result