# src/tools/trend_tools.py
from typing import Dict, Any, List
import structlog

from mcp.types import Tool, TextContent
from src.clients.go_client import GoServiceClient
from src.services.data_processor import DataProcessor
from src.utils.error_handler import handle_tool_error

logger = structlog.get_logger()

class TrendTools:
    """è¶‹åŠ¿åˆ†æå·¥å…· - MVPç‰ˆæœ¬"""
    
    def __init__(self, go_client: GoServiceClient, data_processor: DataProcessor):
        self.go_client = go_client
        self.data_processor = data_processor
    
    def get_tools(self) -> List[Tool]:
        """è·å–å·¥å…·å®šä¹‰"""
        return [
            Tool(
                name="get_trending_papers",
                description="è·å–çƒ­é—¨è®ºæ–‡è¶‹åŠ¿",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "time_window": {
                            "type": "string",
                            "enum": ["week", "month", "year"],
                            "default": "month",
                            "description": "æ—¶é—´çª—å£"
                        },
                        "limit": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 50,
                            "default": 20,
                            "description": "è¿”å›æ•°é‡"
                        }
                    }
                }
            ),
            Tool(
                name="get_top_keywords",
                description="è·å–çƒ­é—¨ç ”ç©¶è¯é¢˜",
                inputSchema={
                    "type": "object",
                    "properties": {}
                }
            ),
            Tool(
                name="analyze_domain_trends",
                description="åˆ†æç‰¹å®šé¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "domain": {
                            "type": "string",
                            "description": "ç ”ç©¶é¢†åŸŸå…³é”®è¯"
                        },
                        "time_range": {
                            "type": "string",
                            "description": "æ—¶é—´èŒƒå›´ï¼Œå¦‚'2020-2024'"
                        },
                        "analysis_type": {
                            "type": "string",
                            "enum": ["publication_trend", "author_activity", "keyword_evolution"],
                            "default": "publication_trend",
                            "description": "åˆ†æç±»å‹"
                        }
                    },
                    "required": ["domain", "time_range"]
                }
            )
        ]
    
    @handle_tool_error
    async def get_trending_papers(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·å–çƒ­é—¨è®ºæ–‡è¶‹åŠ¿å·¥å…·"""
        time_window = arguments.get("time_window", "month")
        limit = arguments.get("limit", 20)
        
        logger.info(
            "Getting trending papers",
            time_window=time_window,
            limit=limit
        )
        
        try:
            raw_result = await self.go_client.get_trending_papers(
                time_window=time_window,
                limit=limit
            )
            
            content = self._format_trending_papers(raw_result, time_window)
            return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Get trending papers failed", error=str(e))
            return [TextContent(type="text", text=f"è·å–çƒ­é—¨è®ºæ–‡å¤±è´¥: {str(e)}")]
    
    @handle_tool_error
    async def get_top_keywords(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·å–çƒ­é—¨è¯é¢˜å·¥å…·"""
        logger.info("Getting top keywords")
        
        try:
            raw_result = await self.go_client.get_top_keywords()
            
            content = self._format_top_keywords(raw_result)
            return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Get top keywords failed", error=str(e))
            return [TextContent(type="text", text=f"è·å–çƒ­é—¨è¯é¢˜å¤±è´¥: {str(e)}")]
    
    @handle_tool_error
    async def analyze_domain_trends(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """åˆ†æé¢†åŸŸè¶‹åŠ¿å·¥å…·"""
        domain = arguments["domain"]
        time_range = arguments["time_range"]
        analysis_type = arguments.get("analysis_type", "publication_trend")
        
        logger.info(
            "Analyzing domain trends",
            domain=domain,
            time_range=time_range,
            analysis_type=analysis_type
        )
        
        try:
            content = await self._analyze_domain_trends(domain, time_range, analysis_type)
            return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Analyze domain trends failed", error=str(e))
            return [TextContent(type="text", text=f"åˆ†æé¢†åŸŸè¶‹åŠ¿å¤±è´¥: {str(e)}")]
    
    def _format_trending_papers(self, raw_result: Dict[str, Any], time_window: str) -> str:
        """æ ¼å¼åŒ–çƒ­é—¨è®ºæ–‡"""
        trending_papers = raw_result.get('trending_papers', [])
        count = raw_result.get('count', len(trending_papers))
        
        content = f"# çƒ­é—¨è®ºæ–‡è¶‹åŠ¿\n\n"
        content += f"**æ—¶é—´çª—å£**: {time_window}\n"
        content += f"**è®ºæ–‡æ•°é‡**: {count}\n\n"
        
        if not trending_papers:
            content += "æš‚æ— çƒ­é—¨è®ºæ–‡æ•°æ®ã€‚\n"
            return content
        
        content += "## çƒ­é—¨è®ºæ–‡æ’è¡Œ\n\n"
        
        for i, paper in enumerate(trending_papers, 1):
            content += f"### {i}. {paper.get('title', 'Unknown Title')}\n\n"
            
            # ä½œè€…ä¿¡æ¯
            if paper.get('authors'):
                if isinstance(paper['authors'], list):
                    authors = ", ".join(paper['authors'][:3])
                    if len(paper['authors']) > 3:
                        authors += f" ç­‰ ({len(paper['authors'])}ä½ä½œè€…)"
                else:
                    authors = str(paper['authors'])
                content += f"**ä½œè€…**: {authors}\n"
            
            # çƒ­åº¦æŒ‡æ ‡
            if paper.get('popularity_score'):
                content += f"**çƒ­åº¦è¯„åˆ†**: {paper['popularity_score']:.2f}\n"
            
            if paper.get('citations') is not None:
                content += f"**å¼•ç”¨æ•°**: {paper['citations']}\n"
            
            if paper.get('published_at'):
                content += f"**å‘è¡¨æ—¶é—´**: {paper['published_at'][:10]}\n"
            
            # å…³é”®è¯
            if paper.get('keywords'):
                keywords = ", ".join(paper['keywords'][:5])
                content += f"**å…³é”®è¯**: {keywords}\n"
            
            # æ‘˜è¦
            if paper.get('abstract'):
                abstract = paper['abstract'][:150] + "..." if len(paper['abstract']) > 150 else paper['abstract']
                content += f"**æ‘˜è¦**: {abstract}\n"
            
            content += "\n---\n\n"
        
        return content
    
    def _format_top_keywords(self, raw_result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–çƒ­é—¨è¯é¢˜"""
        keywords = raw_result.get('keywords', [])
        count = raw_result.get('count', len(keywords))
        
        content = f"# çƒ­é—¨ç ”ç©¶è¯é¢˜\n\n"
        content += f"**è¯é¢˜æ•°é‡**: {count}\n\n"
        
        if not keywords:
            content += "æš‚æ— çƒ­é—¨è¯é¢˜æ•°æ®ã€‚\n"
            return content
        
        content += "## çƒ­é—¨è¯é¢˜æ’è¡Œ\n\n"
        
        # æŒ‰è®ºæ–‡æ•°é‡æ’åº
        sorted_keywords = sorted(keywords, key=lambda x: x.get('paper_count', 0), reverse=True)
        
        for i, keyword in enumerate(sorted_keywords[:20], 1):  # æ˜¾ç¤ºå‰20ä¸ª
            content += f"{i}. **{keyword.get('keyword', 'Unknown')}**"
            
            paper_count = keyword.get('paper_count', 0)
            content += f" - {paper_count} ç¯‡è®ºæ–‡"
            
            # è®¡ç®—çƒ­åº¦ç­‰çº§
            if paper_count >= 100:
                heat_level = "ğŸ”¥ğŸ”¥ğŸ”¥ è¶…çƒ­é—¨"
            elif paper_count >= 50:
                heat_level = "ğŸ”¥ğŸ”¥ çƒ­é—¨"
            elif paper_count >= 20:
                heat_level = "ğŸ”¥ æ´»è·ƒ"
            else:
                heat_level = "ğŸ“Š æ–°å…´"
            
            content += f" ({heat_level})\n"
        
        # è¯é¢˜åˆ†ç±»ç»Ÿè®¡
        content += "\n## è¯é¢˜çƒ­åº¦åˆ†å¸ƒ\n"
        super_hot = len([k for k in keywords if k.get('paper_count', 0) >= 100])
        hot = len([k for k in keywords if 50 <= k.get('paper_count', 0) < 100])
        active = len([k for k in keywords if 20 <= k.get('paper_count', 0) < 50])
        emerging = len([k for k in keywords if k.get('paper_count', 0) < 20])
        
        content += f"- ğŸ”¥ğŸ”¥ğŸ”¥ è¶…çƒ­é—¨è¯é¢˜: {super_hot} ä¸ª\n"
        content += f"- ğŸ”¥ğŸ”¥ çƒ­é—¨è¯é¢˜: {hot} ä¸ª\n"
        content += f"- ğŸ”¥ æ´»è·ƒè¯é¢˜: {active} ä¸ª\n"
        content += f"- ğŸ“Š æ–°å…´è¯é¢˜: {emerging} ä¸ª\n"
        
        return content
    
    async def _analyze_domain_trends(self, domain: str, time_range: str, analysis_type: str) -> str:
        """åˆ†æé¢†åŸŸè¶‹åŠ¿"""
        content = f"# {domain} é¢†åŸŸè¶‹åŠ¿åˆ†æ\n\n"
        content += f"**åˆ†ææ—¶é—´**: {time_range}\n"
        content += f"**åˆ†æç±»å‹**: {analysis_type}\n\n"
        
        try:
            # è§£ææ—¶é—´èŒƒå›´
            years = time_range.split("-")
            start_year = int(years[0]) if len(years) > 0 else 2020
            end_year = int(years[1]) if len(years) > 1 else 2024
            
            if analysis_type == "publication_trend":
                content += await self._analyze_publication_trend(domain, start_year, end_year)
            elif analysis_type == "author_activity":
                content += await self._analyze_author_activity(domain, start_year, end_year)
            elif analysis_type == "keyword_evolution":
                content += await self._analyze_keyword_evolution(domain, start_year, end_year)
            
        except Exception as e:
            content += f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\n"
        
        return content
    
    async def _analyze_publication_trend(self, domain: str, start_year: int, end_year: int) -> str:
        """åˆ†æå‘è¡¨è¶‹åŠ¿"""
        content = "## å‘è¡¨è¶‹åŠ¿åˆ†æ\n\n"
        
        yearly_data = []
        total_papers = 0
        total_citations = 0
        
        for year in range(start_year, end_year + 1):
            try:
                # æœç´¢è¯¥å¹´ä»½è¯¥é¢†åŸŸçš„è®ºæ–‡
                papers_result = await self.go_client.search_papers(
                    query=domain,
                    filters={"year": year},
                    limit=100
                )
                
                papers = papers_result.get("papers", [])
                paper_count = len(papers)
                citation_count = sum(paper.get("citations", 0) for paper in papers)
                
                yearly_data.append({
                    "year": year,
                    "papers": paper_count,
                    "citations": citation_count
                })
                
                total_papers += paper_count
                total_citations += citation_count
                
            except Exception as e:
                logger.warning(f"Failed to get data for year {year}", error=str(e))
                yearly_data.append({"year": year, "papers": 0, "citations": 0})
        
        # æ˜¾ç¤ºå¹´åº¦æ•°æ®
        content += "### å¹´åº¦å‘è¡¨ç»Ÿè®¡\n\n"
        content += "```\n"
        content += f"{'å¹´ä»½':<8} {'è®ºæ–‡æ•°':<8} {'å¼•ç”¨æ•°':<10} {'è¶‹åŠ¿'}\n"
        content += "-" * 35 + "\n"
        
        prev_papers = None
        for data in yearly_data:
            trend_indicator = ""
            if prev_papers is not None:
                if data["papers"] > prev_papers:
                    trend_indicator = "â†—"
                elif data["papers"] < prev_papers:
                    trend_indicator = "â†˜"
                else:
                    trend_indicator = "â†’"
            
            content += f"{data['year']:<8} {data['papers']:<8} {data['citations']:<10} {trend_indicator}\n"
            prev_papers = data["papers"]
        
        content += "```\n\n"
        
        # è¶‹åŠ¿æ€»ç»“
        if len(yearly_data) >= 2:
            first_year_papers = yearly_data[0]["papers"]
            last_year_papers = yearly_data[-1]["papers"]
            
            if first_year_papers > 0:
                growth_rate = ((last_year_papers - first_year_papers) / first_year_papers) * 100
            else:
                growth_rate = 0
            
            content += "### è¶‹åŠ¿æ€»ç»“\n"
            content += f"- æ€»è®ºæ–‡æ•°: {total_papers}\n"
            content += f"- æ€»å¼•ç”¨æ•°: {total_citations}\n"
            content += f"- å¹´å‡è®ºæ–‡æ•°: {total_papers / len(yearly_data):.1f}\n"
            content += f"- å¢é•¿ç‡: {growth_rate:+.1f}%\n"
            
            if growth_rate > 20:
                content += "- å‘å±•æ€åŠ¿: å¿«é€Ÿå¢é•¿ ğŸ“ˆ\n"
            elif growth_rate > 0:
                content += "- å‘å±•æ€åŠ¿: ç¨³å®šå¢é•¿ ğŸ“Š\n"
            elif growth_rate > -20:
                content += "- å‘å±•æ€åŠ¿: åŸºæœ¬ç¨³å®š â¡ï¸\n"
            else:
                content += "- å‘å±•æ€åŠ¿: ä¸‹é™è¶‹åŠ¿ ğŸ“‰\n"
        
        return content
    
    async def _analyze_author_activity(self, domain: str, start_year: int, end_year: int) -> str:
        """åˆ†æä½œè€…æ´»è·ƒåº¦"""
        content = "## ä½œè€…æ´»è·ƒåº¦åˆ†æ\n\n"
        
        try:
                    # è·å–è¯¥é¢†åŸŸçš„è®ºæ–‡å’Œä½œè€…
            papers_result = await self.go_client.search_papers(
                query=domain,
                filters={"year_from": start_year, "year_to": end_year},
                limit=100
            )
            
            papers = papers_result.get("papers", [])
            
            # ç»Ÿè®¡ä½œè€…æ´»è·ƒåº¦
            author_stats = {}
            for paper in papers:
                for author in paper.get("authors", []):
                    author_name = author.get("name", "")
                    if author_name:
                        if author_name not in author_stats:
                            author_stats[author_name] = {
                                "name": author_name,
                                "paper_count": 0,
                                "total_citations": 0,
                                "affiliation": author.get("affiliation", ""),
                                "author_id": author.get("id", "")
                            }
                        author_stats[author_name]["paper_count"] += 1
                        author_stats[author_name]["total_citations"] += paper.get("citations", 0)
            
            # æ’åºå¹¶å–å‰20
            top_authors = sorted(
                author_stats.values(),
                key=lambda x: x["paper_count"],
                reverse=True
            )[:20]
            
            content += f"### æ´»è·ƒä½œè€…æ’è¡Œ (åŸºäº{len(papers)}ç¯‡è®ºæ–‡)\n\n"
            
            for i, author in enumerate(top_authors, 1):
                content += f"{i}. **{author['name']}**\n"
                content += f"   - è®ºæ–‡æ•°: {author['paper_count']}\n"
                content += f"   - æ€»å¼•ç”¨æ•°: {author['total_citations']}\n"
                if author['affiliation']:
                    content += f"   - æœºæ„: {author['affiliation']}\n"
                content += "\n"
            
            # æ´»è·ƒåº¦ç»Ÿè®¡
            content += "### ä½œè€…æ´»è·ƒåº¦åˆ†å¸ƒ\n"
            high_activity = len([a for a in author_stats.values() if a["paper_count"] >= 5])
            medium_activity = len([a for a in author_stats.values() if 2 <= a["paper_count"] < 5])
            low_activity = len([a for a in author_stats.values() if a["paper_count"] == 1])
            
            content += f"- é«˜æ´»è·ƒåº¦ä½œè€… (â‰¥5ç¯‡): {high_activity} ä½\n"
            content += f"- ä¸­ç­‰æ´»è·ƒåº¦ä½œè€… (2-4ç¯‡): {medium_activity} ä½\n"
            content += f"- ä½æ´»è·ƒåº¦ä½œè€… (1ç¯‡): {low_activity} ä½\n"
            
        except Exception as e:
            content += f"åˆ†æä½œè€…æ´»è·ƒåº¦æ—¶å‡ºé”™: {str(e)}\n"
        
        return content
    
    async def _analyze_keyword_evolution(self, domain: str, start_year: int, end_year: int) -> str:
        """åˆ†æå…³é”®è¯æ¼”åŒ–"""
        content = "## å…³é”®è¯æ¼”åŒ–åˆ†æ\n\n"
        
        try:
            # è·å–ä¸åŒæ—¶æœŸçš„å…³é”®è¯
            mid_year = (start_year + end_year) // 2
            
            # æ—©æœŸå…³é”®è¯
            early_papers = await self.go_client.search_papers(
                query=domain,
                filters={"year_from": start_year, "year_to": mid_year},
                limit=50
            )
            
            # è¿‘æœŸå…³é”®è¯
            recent_papers = await self.go_client.search_papers(
                query=domain,
                filters={"year_from": mid_year + 1, "year_to": end_year},
                limit=50
            )
            
            # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
            early_keywords = {}
            recent_keywords = {}
            
            for paper in early_papers.get("papers", []):
                for keyword in paper.get("keywords", []):
                    early_keywords[keyword] = early_keywords.get(keyword, 0) + 1
            
            for paper in recent_papers.get("papers", []):
                for keyword in paper.get("keywords", []):
                    recent_keywords[keyword] = recent_keywords.get(keyword, 0) + 1
            
            # åˆ†æå…³é”®è¯å˜åŒ–
            content += f"### å…³é”®è¯æ¼”åŒ– ({start_year}-{mid_year} vs {mid_year+1}-{end_year})\n\n"
            
            # æ–°å…´å…³é”®è¯
            emerging_keywords = []
            for keyword, count in recent_keywords.items():
                if keyword not in early_keywords and count >= 2:
                    emerging_keywords.append((keyword, count))
            
            emerging_keywords.sort(key=lambda x: x[1], reverse=True)
            
            if emerging_keywords:
                content += "#### ğŸ†• æ–°å…´å…³é”®è¯\n"
                for keyword, count in emerging_keywords[:10]:
                    content += f"- **{keyword}** ({count} æ¬¡)\n"
                content += "\n"
            
            # æŒç»­çƒ­é—¨å…³é”®è¯
            persistent_keywords = []
            for keyword in early_keywords:
                if keyword in recent_keywords:
                    early_count = early_keywords[keyword]
                    recent_count = recent_keywords[keyword]
                    growth = recent_count - early_count
                    persistent_keywords.append((keyword, early_count, recent_count, growth))
            
            persistent_keywords.sort(key=lambda x: x[2], reverse=True)
            
            if persistent_keywords:
                content += "#### ğŸ”¥ æŒç»­çƒ­é—¨å…³é”®è¯\n"
                for keyword, early, recent, growth in persistent_keywords[:10]:
                    trend = "â†—" if growth > 0 else "â†˜" if growth < 0 else "â†’"
                    content += f"- **{keyword}**: {early} â†’ {recent} {trend}\n"
                content += "\n"
            
            # è¡°é€€å…³é”®è¯
            declining_keywords = []
            for keyword, count in early_keywords.items():
                if keyword not in recent_keywords and count >= 2:
                    declining_keywords.append((keyword, count))
            
            declining_keywords.sort(key=lambda x: x[1], reverse=True)
            
            if declining_keywords:
                content += "#### ğŸ“‰ è¡°é€€å…³é”®è¯\n"
                for keyword, count in declining_keywords[:5]:
                    content += f"- **{keyword}** (æ—©æœŸ: {count} æ¬¡)\n"
                content += "\n"
            
            # å…³é”®è¯å¤šæ ·æ€§åˆ†æ
            total_early = len(early_keywords)
            total_recent = len(recent_keywords)
            overlap = len(set(early_keywords.keys()) & set(recent_keywords.keys()))
            
            content += "#### å…³é”®è¯å¤šæ ·æ€§\n"
            content += f"- æ—©æœŸå…³é”®è¯æ€»æ•°: {total_early}\n"
            content += f"- è¿‘æœŸå…³é”®è¯æ€»æ•°: {total_recent}\n"
            content += f"- é‡å å…³é”®è¯: {overlap}\n"
            if total_early > 0:
                continuity = (overlap / total_early) * 100
                content += f"- ç ”ç©¶è¿ç»­æ€§: {continuity:.1f}%\n"
            
        except Exception as e:
            content += f"åˆ†æå…³é”®è¯æ¼”åŒ–æ—¶å‡ºé”™: {str(e)}\n"
        
        return content

