# src/tools/trend_tools.py
from typing import Dict, Any, List
import structlog

from mcp.types import Tool,TextContent
from src.clients.go_client import GoServiceClient
from src.services.data_processor import DataProcessor
from src.models.data_models import TrendAnalysis, TrendData
from src.utils.error_handler import handle_tool_error

logger = structlog.get_logger()

class TrendTools:
    """è¶‹åŠ¿åˆ†æå·¥å…· - MVPç‰ˆæœ¬"""
    
    def __init__(self, go_client: GoServiceClient, data_processor: DataProcessor):
        self.go_client = go_client
        self.data_processor = data_processor
    
    def get_tools(self) -> List[Tool]:
        """è·å–å·¥å…·å®šä¹‰"""
        return [
            Tool(
                name="get_research_trends",
                description="è·å–ç ”ç©¶é¢†åŸŸè¶‹åŠ¿åˆ†æ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "domain": {
                            "type": "string",
                            "description": "ç ”ç©¶é¢†åŸŸ"
                        },
                        "time_range": {
                            "type": "string",
                            "description": "æ—¶é—´èŒƒå›´ï¼Œå¦‚'2020-2024'"
                        },
                        "metrics": {
                            "type": "array",
                            "items": {"type": "string"},
                            "default": ["publication_count", "citation_count"],
                            "description": "åˆ†ææŒ‡æ ‡"
                        },
                        "granularity": {
                            "type": "string",
                            "enum": ["month", "quarter", "year"],
                            "default": "year",
                            "description": "æ—¶é—´ç²’åº¦"
                        }
                    },
                    "required": ["domain", "time_range"]
                }
            ),
            Tool(
                name="analyze_research_landscape",
                description="åˆ†æç ”ç©¶é¢†åŸŸå…¨æ™¯",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "domain": {
                            "type": "string",
                            "description": "ç ”ç©¶é¢†åŸŸ"
                        },
                        "analysis_dimensions": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "åˆ†æç»´åº¦"
                        }
                    },
                    "required": ["domain", "analysis_dimensions"]
                }
            )
        ]
    
    @handle_tool_error
    async def get_research_trends(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """è·å–ç ”ç©¶è¶‹åŠ¿å·¥å…·"""
        domain = arguments["domain"]
        time_range = arguments["time_range"]
        metrics = arguments.get("metrics", ["publication_count", "citation_count"])
        granularity = arguments.get("granularity", "year")
        
        logger.info(
            "Getting research trends",
            domain=domain,
            time_range=time_range,
            metrics=metrics
        )
        
        try:
            raw_result = await self.go_client.get_research_trends(
                domain=domain,
                time_range=time_range,
                metrics=metrics,
                granularity=granularity
            )
            
            # è§£æè¶‹åŠ¿æ•°æ®
            trend_analysis = self._parse_trend_data(raw_result, domain, time_range, metrics)
            
            content = self._format_trend_analysis(trend_analysis)
            return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Get research trends failed", error=str(e))
            return [TextContent(type="text", text=f"è·å–ç ”ç©¶è¶‹åŠ¿å¤±è´¥: {str(e)}")]
    
    @handle_tool_error
    async def analyze_research_landscape(self, arguments: Dict[str, Any]) -> List[TextContent]:
        """åˆ†æç ”ç©¶é¢†åŸŸå…¨æ™¯å·¥å…·"""
        domain = arguments["domain"]
        analysis_dimensions = arguments["analysis_dimensions"]
        
        logger.info(
            "Analyzing research landscape",
            domain=domain,
            dimensions=analysis_dimensions
        )
        
        try:
            raw_result = await self.go_client.analyze_research_landscape(
                domain=domain,
                analysis_dimensions=analysis_dimensions
            )
            
            content = self._format_landscape_analysis(raw_result, domain)
            return [TextContent(type="text", text=content)]
            
        except Exception as e:
            logger.error("Analyze research landscape failed", error=str(e))
            return [TextContent(type="text", text=f"åˆ†æç ”ç©¶é¢†åŸŸå…¨æ™¯å¤±è´¥: {str(e)}")]
    
    def _parse_trend_data(
        self, 
        raw_data: Dict[str, Any], 
        domain: str, 
        time_range: str, 
        metrics: List[str]
    ) -> TrendAnalysis:
        """è§£æè¶‹åŠ¿æ•°æ®"""
        data_points = []
        
        for raw_point in raw_data.get("data_points", []):
            point = TrendData(
                time_period=raw_point["time_period"],
                value=raw_point["value"],
                label=raw_point.get("label")
            )
            data_points.append(point)
        
        return TrendAnalysis(
            domain=domain,
            time_range=time_range,
            data_points=data_points,
            metrics=metrics
        )
    
    def _format_trend_analysis(self, trend_analysis: TrendAnalysis) -> str:
        """æ ¼å¼åŒ–è¶‹åŠ¿åˆ†æç»“æœ"""
        content = f"# ç ”ç©¶è¶‹åŠ¿åˆ†æ\n\n"
        content += f"**ç ”ç©¶é¢†åŸŸ**: {trend_analysis.domain}\n"
        content += f"**æ—¶é—´èŒƒå›´**: {trend_analysis.time_range}\n"
        content += f"**åˆ†ææŒ‡æ ‡**: {', '.join(trend_analysis.metrics)}\n\n"
        
        if not trend_analysis.data_points:
            content += "æš‚æ— è¶‹åŠ¿æ•°æ®ã€‚\n"
            return content
        
        content += "## è¶‹åŠ¿æ•°æ®\n\n"
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_points = sorted(trend_analysis.data_points, key=lambda x: x.time_period)
        
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬å›¾è¡¨
        content += "```\n"
        content += f"{'æ—¶é—´':<12} {'æ•°å€¼':<10} {'è¶‹åŠ¿'}\n"
        content += "-" * 30 + "\n"
        
        prev_value = None
        for point in sorted_points:
            trend_indicator = ""
            if prev_value is not None:
                if point.value > prev_value:
                    trend_indicator = "â†—"
                elif point.value < prev_value:
                    trend_indicator = "â†˜"
                else:
                    trend_indicator = "â†’"
            
            content += f"{point.time_period:<12} {point.value:<10.1f} {trend_indicator}\n"
            prev_value = point.value
        
        content += "```\n\n"
        
        # è¶‹åŠ¿æ€»ç»“
        if len(sorted_points) >= 2:
            first_value = sorted_points[0].value
            last_value = sorted_points[-1].value
            change_rate = ((last_value - first_value) / first_value) * 100 if first_value != 0 else 0
            
            content += "## è¶‹åŠ¿æ€»ç»“\n"
            content += f"- èµ·å§‹å€¼: {first_value:.1f}\n"
            content += f"- ç»“æŸå€¼: {last_value:.1f}\n"
            content += f"- å˜åŒ–ç‡: {change_rate:+.1f}%\n"
            
            if change_rate > 10:
                content += "- è¶‹åŠ¿: æ˜¾è‘—ä¸Šå‡ ğŸ“ˆ\n"
            elif change_rate > 0:
                content += "- è¶‹åŠ¿: æ¸©å’Œä¸Šå‡ ğŸ“Š\n"
            elif change_rate > -10:
                content += "- è¶‹åŠ¿: åŸºæœ¬ç¨³å®š â¡ï¸\n"
            else:
                content += "- è¶‹åŠ¿: æ˜æ˜¾ä¸‹é™ ğŸ“‰\n"
        
        return content
    
    def _format_landscape_analysis(self, raw_data: Dict[str, Any], domain: str) -> str:
        """æ ¼å¼åŒ–é¢†åŸŸå…¨æ™¯åˆ†æç»“æœ"""
        content = f"# ç ”ç©¶é¢†åŸŸå…¨æ™¯åˆ†æ\n\n"
        content += f"**ç ”ç©¶é¢†åŸŸ**: {domain}\n\n"
        
        # çƒ­é—¨ä¸»é¢˜
        if "hot_topics" in raw_data:
            content += "## çƒ­é—¨ç ”ç©¶ä¸»é¢˜\n"
            for i, topic in enumerate(raw_data["hot_topics"][:10], 1):
                content += f"{i}. **{topic['name']}**"
                if "score" in topic:
                    content += f" (çƒ­åº¦: {topic['score']:.2f})"
                if "paper_count" in topic:
                    content += f" (è®ºæ–‡æ•°: {topic['paper_count']})"
                content += "\n"
            content += "\n"
        
        # é¡¶çº§æœºæ„
        if "top_institutions" in raw_data:
            content += "## é¡¶çº§ç ”ç©¶æœºæ„\n"
            for i, inst in enumerate(raw_data["top_institutions"][:10], 1):
                content += f"{i}. **{inst['name']}**"
                if "paper_count" in inst:
                    content += f" (è®ºæ–‡æ•°: {inst['paper_count']})"
                if "citation_count" in inst:
                    content += f" (å¼•ç”¨æ•°: {inst['citation_count']})"
                content += "\n"
            content += "\n"
        
        # æ´»è·ƒä½œè€…
        if "active_authors" in raw_data:
            content += "## æ´»è·ƒç ”ç©¶è€…\n"
            for i, author in enumerate(raw_data["active_authors"][:10], 1):
                content += f"{i}. **{author['name']}**"
                if "affiliation" in author:
                    content += f" ({author['affiliation']})"
                if "paper_count" in author:
                    content += f" - {author['paper_count']} ç¯‡è®ºæ–‡"
                content += "\n"
            content += "\n"
        
        # é‡è¦ä¼šè®®/æœŸåˆŠ
        if "top_venues" in raw_data:
            content += "## é‡è¦å‘è¡¨å¹³å°\n"
            for i, venue in enumerate(raw_data["top_venues"][:10], 1):
                content += f"{i}. **{venue['name']}**"
                if "type" in venue:
                    content += f" ({venue['type']})"
                if "paper_count" in venue:
                    content += f" - {venue['paper_count']} ç¯‡è®ºæ–‡"
                content += "\n"
            content += "\n"
        
        # æ–°å…´è¶‹åŠ¿
        if "emerging_trends" in raw_data:
            content += "## æ–°å…´ç ”ç©¶è¶‹åŠ¿\n"
            for i, trend in enumerate(raw_data["emerging_trends"][:5], 1):
                content += f"{i}. **{trend['name']}**"
                if "growth_rate" in trend:
                    content += f" (å¢é•¿ç‡: {trend['growth_rate']:+.1f}%)"
                if "description" in trend:
                    content += f"\n   {trend['description']}"
                content += "\n"
            content += "\n"
        
        return content

